## Step 1: Environment Setup

To ensure smooth operation within a Jupyter Notebook, we first set up our environment:

```{python}
import sys
 # Adding parent directory to the paths. This allows you to load
 # packages in folders located one level before.
sys.path.append("../")

# IPython magic commands for auto-reloading modules
%load_ext autoreload
%autoreload 2
```


## Step 2: Parsing Instances

The parsing process transforms an instance file of a certain type, which follows an specific convention to list information (e.g., locations, demands, fleet configuration, costs, etc.), into an instance object whose information can be formatted as an input to the solver.

```{mermaid}
graph LR;
    A1("Instance Type 1") --> B1("Parser \nInstance Type 1");
    A2("Instance Type 2") --> B2("Parser \nInstance Type 2");
    AN("Instance Type N") --> BN("Parser \nInstance Type N");
    B1 --> C(Instance\nObject);
    B2 --> C;
    BN --> C;
    C --> D(Solver)
    D --> E(Solution)
```

Here is how an instance class can be constructed:

```python
class Instance:
    def __init__(
        self,
        vehicles: list[Vehicle],
        requests: list[Request],
        nodes: list[NodeInfo],
        config: InstanceConfig,
        instance_filepath: str,
        instance_parser: str,
    ):

@dataclass
class InstanceConfig:
    n_vehicles: int
    n_customers: int
    vehicle_capacity: int
    maximum_driving_time_min: int = None
    time_horizon_min: int = None
    maximum_ride_time_min: int = None
    n_depots: int = 1
```

Notice that `InstanceConfig` is a dataclass. Dataclasses are a simpler way of representing immutable information.

The rest of the instance data is also parsed into lists of  request, vehicles, and node objects.

Below, an example of how instances for the *multi-depot vehicle routing problem with pick and deliviries* (MDVRPPD) are parsed.

```{python}
from pathlib import Path
from pprint import pprint

# Parser to load and parse DARP instance data
import src.data.parser as parser

folder_path = Path("../data/raw/mdvrppdtw/")

instances_files = [
    "vrppd_13-3-5.txt",
    "vrppd_23-3-10.txt",
    "vrppd_33-3-15.txt",
]

# The instances are saved into a dictionary with
# keys corresponding to the instance name
instances = {}

for instance_file in instances_files:

    # Create file path
    instance_filepath = folder_path / instance_file
    print(f"\n## Processing instance at '{instance_filepath}'")

    # Create an Instance object. The parser, is a function
    # created to read files of this instance type. When working
    # with instances from different sources, you build a parser
    # for each source so instance files are read correctly.
    instance_obj = parser.parse_instance_from_filepath(
        instance_filepath,
        instance_parser=parser.PARSER_TYPE_MVRPPDTW
    )

    instances[instance_file[:-4]] = instance_obj
```

## Step 3: Visualizing instances (sanity check)

With an instance object, you can manipulate the information in different ways.

For example, consider the instance `vrppd_13-3-5`:

```
/* Cardinality of the grid */
13
/* Cardinality of pick-up locations */
5
/* Cardinality of delivery locations */
5
/* Cardinality of depots */
3
/* Total number of trucks */
1	
/* Capacity of trucks */
200
/* Maximum working hours */
8
/* Revenue of selling an order [e/kg] */
50
/* Depots */
/* node_ID, x_coord, y_coord, demand, tw_start, tw_end */
1	40	50	0	0	8
2	27	62	0	0	8
3	73	17	0	0	8
/* Pick up locations */
/* node_ID, x_coord, y_coord, demand, tw_start, tw_end */			
4	25	85	20	3	7
5	20	85	20	3	7
6	15	75	20	2	6
7	15	80	10	1	5
8	10	35	20	1	5
/* Delivery locations */
/* node_ID, x_coord, y_coord, demand, tw_start, tw_end */				
9	22	75	-20	4	8
10	22	85	-20	4	8
11	20	80	-20	3	7
12	18	75	-10	2	6
13	5	35	-20	2	6
```

Below, this instance is shown using different structures to facility modeling and sanity checks.

### DataFrame of node data

DataFrame view of the instance `vrppd_13-3-5`. Notice that nodes have aliases to facilitate reading. The alias of dropoff node corresponding to a pickup node labelled `N` is `N*`. This way, pickup and dropoff pairs can be determined easier.

Additionally, to facilitate modeling, the depot nodes are replicated.
This way, we wave two types of depots: origin (`O_DEPOT`) and destination (`D_DEPOT`).
Doing so guarantees that time-related constraints are easier to model.
For example, every node is associated with a single arrival time.
By duplicating depots, we guarantee that the arrival time at an origin depot corresponding to departure time of this depot, whereas the arrival time at the corresponding destination depot corresonds to the arrival time at the depot after the journey has completed.

```{python}
# | tbl-cap: To facilitate modeling, depots are replicated to differentiate between origin (`O_DEPOT`) and destination (`D_DEPOT`) depots. All vehicles start from an origin depot and finish at a destination depot.

# Displaying the instance data as a DataFrame for verification
df_instance = instances["vrppd_13-3-5"].nodeset_df
df_instance
```


### Textual representation 

The instance object can be print in different forms.

For example, here is a print of the `InstanceConfig` object:

```{python}
pprint(instances["vrppd_13-3-5"].config)
```

And here a print of the `Instance` object:

```{python}
print(instances["vrppd_13-3-5"])
```

Such customized prints can be done by modifying the dunder method `__str__` of the objects `Instance` and `InstanceConfig`.

### Plot of node data

To check if instances are read correctly, plot them.

```{python}
# | fig-cap: Node distribution of the instances.
import seaborn as sns
import matplotlib.pyplot as plt

# Destination depots are filtered since they are replicas
# of origin depots
df = instance_obj.nodeset_df[instance_obj.nodeset_df["node_type"]!='D_DEPOT']

# Create subplots for each instance
fig, axs = plt.subplots(
    1,
    len(instances),
    figsize=(5 * len(instances), 5))

# Iterate over instances and plot scatter plots
for i, (instance_file, instance_obj) in enumerate(instances.items()):
    ax = axs[i]
    subfig_title = f"{instance_file}\n({instance_obj.config.label})"
    ax.set_title(subfig_title)
    sns.scatterplot(
        data=df,
        x="x",
        y="y",
        hue="node_type",
        s=150,
        ax=ax)
    
    # Add text annotations for the nodes
    for index, row in df.iterrows():
        ax.text(
            row['x'], row['y'],
            str(row['alias']),
            fontsize=7,
            ha='center',
            va='center',
            color='black',
            fontfamily="Consolas",
        )
```


## Step 4: Reading the scenarios

The scenario settings are data saved as `.json`, in folder `{python} folder_path`.

A scenario holds information about the experiment including:

- Whether to consider flexible depots (i.e., vehicles do not need to leave and return to same depot.).
- Maximum driving time for trucks is taken into account.
- Customers can be rejected.
- The objective function.
- The instances considered in the scenario.
- The description (e.g., the question statement.).

These scenarios are read into `Scenario` objects and stored in a dictionary.

```{python}
from pprint import pprint
from src.data.scenario import Scenario

scenarios = Scenario.read(folder_path / "scenarios.json")
pprint(scenarios)
```

## Step 5: Run scenarios and save results

We loop over all the scenarios and save the main KPIs.

```{python}
import os
import numpy as np
import pandas as pd
from pprint import pprint

import src.solution.build as build

# Aggregated results
results_filepath = "../reports/tables/results_scenarios_m1-m6_10minutes.csv"

# Load data if exists
if os.path.exists(results_filepath):
    df_results = pd.read_csv(results_filepath,index_col=False)
else:
    df_results = pd.DataFrame()

test_count = 0
time_limit_min = 10

for k, scenario in scenarios.items():
    
    for config in scenario.generate_scenario_values():
        
        test_count += 1
        print("#### Test", test_count)

        test_label = config.get_test_label(time_limit_min)

        if test_label in df_results["Test ID"].values:
            print(f"### Test {test_label} already executed.")
            continue


        pprint(config)

        df_test = build.build_run(
            config,
            time_limit_min=time_limit_min,
            lp_filepath="../reports/lps",
            log_filepath="../reports/logs",
            )


        pprint(df_test.round(2).to_dict(orient='records')[0])

        # Save test results in DataFrame
        df_results = pd.concat([df_results, df_test])
        
        # Date helps to track the tests
        df_results.to_csv(results_filepath, index=False)
    
```
